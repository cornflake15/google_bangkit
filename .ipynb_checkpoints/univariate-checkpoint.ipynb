{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denpasar Weather Forecasting with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weather dataset\n",
    "\n",
    "Data collected between 2009-2016 every 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>temp_min</th>\n",
       "      <th>temp_max</th>\n",
       "      <th>pressure</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_deg</th>\n",
       "      <th>clouds_all</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt_iso</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1990-01-01 00:00:00</th>\n",
       "      <td>25.82</td>\n",
       "      <td>25.82</td>\n",
       "      <td>25.82</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>86</td>\n",
       "      <td>1.36</td>\n",
       "      <td>225</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-01 01:00:00</th>\n",
       "      <td>26.20</td>\n",
       "      <td>26.20</td>\n",
       "      <td>26.20</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>84</td>\n",
       "      <td>2.09</td>\n",
       "      <td>247</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-01 02:00:00</th>\n",
       "      <td>26.45</td>\n",
       "      <td>26.45</td>\n",
       "      <td>26.45</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>84</td>\n",
       "      <td>2.44</td>\n",
       "      <td>262</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-01 03:00:00</th>\n",
       "      <td>26.80</td>\n",
       "      <td>26.80</td>\n",
       "      <td>26.80</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>82</td>\n",
       "      <td>2.29</td>\n",
       "      <td>271</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-01 04:00:00</th>\n",
       "      <td>27.04</td>\n",
       "      <td>27.04</td>\n",
       "      <td>27.04</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>82</td>\n",
       "      <td>1.71</td>\n",
       "      <td>274</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      temp  temp_min  temp_max  pressure  humidity  \\\n",
       "dt_iso                                                               \n",
       "1990-01-01 00:00:00  25.82     25.82     25.82    1010.0        86   \n",
       "1990-01-01 01:00:00  26.20     26.20     26.20    1011.0        84   \n",
       "1990-01-01 02:00:00  26.45     26.45     26.45    1011.0        84   \n",
       "1990-01-01 03:00:00  26.80     26.80     26.80    1011.0        82   \n",
       "1990-01-01 04:00:00  27.04     27.04     27.04    1010.0        82   \n",
       "\n",
       "                     wind_speed  wind_deg  clouds_all  \n",
       "dt_iso                                                 \n",
       "1990-01-01 00:00:00        1.36       225          98  \n",
       "1990-01-01 01:00:00        2.09       247          91  \n",
       "1990-01-01 02:00:00        2.44       262          94  \n",
       "1990-01-01 03:00:00        2.29       271          94  \n",
       "1990-01-01 04:00:00        1.71       274          76  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usecols = ['dt_iso', 'temp', 'temp_min', 'temp_max', 'pressure', 'humidity', 'wind_speed', 'wind_deg', 'clouds_all']\n",
    "df = pd.read_csv('openweatherdata-denpasar-1990-2020v0.1.csv', parse_dates=['dt_iso'], index_col='dt_iso', usecols=usecols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264924, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1d3f567fd0c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstattools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madfuller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madf_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeseries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Results of Dickey-Fuller Test:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "def num_data_train(percentage, data_length):\n",
    "    num_data = percentage / 100 * data_length\n",
    "    return int(num_data)\n",
    "\n",
    "def create_time_steps(length):\n",
    "    return list(range(-length, 0))\n",
    "\n",
    "# create a function that return the time windows for the model to train on\n",
    "def univariate_data(dataset, start_index, end_index, history_size, target_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "        \n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i - history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, 1)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        labels.append(dataset[i+target_size])\n",
    "        \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def multi_step_plot(history, true_future, prediction):\n",
    "    num_in = create_time_steps(len(history))\n",
    "    num_out = len(true_future)\n",
    "    \n",
    "    plt.plot(num_in, np.array(history[:, 0]), label='History')\n",
    "    plt.plot(np.arange(num_out), np.array(true_future), 'bo', label='True Future')\n",
    "    if prediction.any():\n",
    "        plt.plot(np.arange(num_out), np.array(prediction), 'ro', label='Predicted Future')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_train_history(history, loss_metric_1, loss_metric_2, title):\n",
    "    lm1 = history.history[loss_metric_1]\n",
    "    lm2 = history.history[loss_metric_2]\n",
    "    \n",
    "    epochs = range(EPOCHS)\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(epochs, lm1, 'b', label=loss_metric_1)\n",
    "    plt.plot(epochs, lm2, 'r', label=loss_metric_2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Function to resampling time series data\n",
    "def data_resample(data, time):\n",
    "    \"\"\"\n",
    "    data: Dataframe\n",
    "    time: Resampling frequencies\n",
    "    \"\"\"\n",
    "    if time == 'hourly':\n",
    "        df_resample = data.resample('H').mean() # hour\n",
    "    elif time == 'daily':\n",
    "        df_resample = data.resample('D').mean() # day\n",
    "    elif time == 'weekly':\n",
    "        df_resample = data.resample('W').mean() # week\n",
    "    elif time == 'monthly':\n",
    "        df_resample = data.resample('M').mean() # month\n",
    "    elif time == 'quarterly':\n",
    "        df_resample = data.resample('Q').mean() # quarter\n",
    "    elif time == 'yearly':\n",
    "        df_resample = data.resample('A').mean() # year\n",
    "    \n",
    "    return df_resample\n",
    "\n",
    "\n",
    "# from statsmodels.tsa.stattools import adfuller\n",
    "# def adf_test(timeseries):\n",
    "#     print('Results of Dickey-Fuller Test:')\n",
    "#     df_test = adfuller(timeseries, autolag='AIC')\n",
    "#     df_output = pd.Series(df_test[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "#     for key, value in df_test[4].items():\n",
    "#         df_output['Critical Value (%s)' % key] = value\n",
    "#     print(df_output)\n",
    "\n",
    "# from statsmodels.tsa.stattools import kpss\n",
    "# def kpss_test(timeseries):\n",
    "#     print('Results of KPSS Test:')\n",
    "#     kpss_test = kpss(timeseries, regression='c', nlags='legacy') # nlags = 'auto' or 'legacy'\n",
    "#     kpss_output = pd.Series(kpss_test[0:3], index=['Test Statistic', 'p-value', 'Lag Used'])\n",
    "#     for key, value in kpss_test[3].items():\n",
    "#         kpss_output['Critical Value (%s)' % key] = value       \n",
    "#     print(kpss_output)\n",
    "    \n",
    "# def test_stationarity(timeseries, info):\n",
    "#     \"\"\"\n",
    "#     timeseries: pandas time series\n",
    "#     info: resampling information, (hourly, daily, weekly, monthly, quarterly, yearly)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Determining rolling statistics\n",
    "#     windowsize = 24\n",
    "#     rolmean = timeseries.rolling(windowsize).mean()\n",
    "#     rolstd = timeseries.rolling(windowsize).mean()\n",
    "    \n",
    "#     # Plot rolling statistics:\n",
    "#     orig = plt.plot(timeseries, color='yellow', label='Original')\n",
    "#     mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "#     std = plt.plot(rolstd, color='black', label='Rolling Std')\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.title('Rolling Mean & Standard Deviation ' + info)\n",
    "#     plt.show(block=False)\n",
    "    \n",
    "#     # Perform Dickey-Fuller Test:\n",
    "#     adf_test(timeseries)\n",
    "    \n",
    "#     print('\\n')\n",
    "    \n",
    "#     # Perform KPSS Test:\n",
    "#     kpss_test(timeseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast a univariate time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc['1990':'2019'][['temp']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_stationarity(df.loc['1990':'2019'][['temp']], 'Hourly Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model using a single feature (temperature)\n",
    "dt_uni = df.loc['1990':'2019'][['temp']]\n",
    "\n",
    "# Differencing, use this if we want make the data stationary with differencing methods\n",
    "# dt_uni['diff'] = dt_uni['temp'] - dt_uni['temp'].shift(1)\n",
    "# dt_uni['diff'] = dt_uni['diff'].fillna(dt_uni['diff'].mean())\n",
    "# dt_uni = dt_uni['diff'].values\n",
    "\n",
    "# Transformation, use this if we want make the data stationary with transformation methods\n",
    "# dt_uni['log'] = np.log(dt_uni['temp'])\n",
    "# dt_uni['log_diff'] = dt_uni['log'] - dt_uni['log'].shift(1)\n",
    "# dt_uni['log_diff'] = dt_uni['log_diff'].fillna(dt_uni['log_diff'].mean())\n",
    "# dt_uni = dt_uni['log_diff']\n",
    "\n",
    "dt_uni = dt_uni.values\n",
    "print(dt_uni[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = num_data_train(60, df.shape[0]) # Split 70% of the data for training\n",
    "print('Length of Data Train: {}'.format(TRAIN_SPLIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "dt_uni_mean = dt_uni[:TRAIN_SPLIT].mean()\n",
    "dt_uni_std = dt_uni[:TRAIN_SPLIT].std()\n",
    "print('Training Data Mean: {}'.format(dt_uni_mean))\n",
    "print('\\nTraining Data Std.Dev: {}'.format(dt_uni_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "# dt = (dt_uni - dt_uni_mean) / dt_uni_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, the model will be given the last **`n`** recorded temperature observation, and needs to learn to predict the temperature at the next **`m`** time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the univariate model\n",
    "past_history = 30 # number of last recorded data (n)\n",
    "future_target = 5 # number of time step (m)\n",
    "\n",
    "x_train, y_train = univariate_data(dt_uni, 0, TRAIN_SPLIT, past_history, future_target)\n",
    "x_val, y_val = univariate_data(dt_uni, TRAIN_SPLIT, None, past_history, future_target)\n",
    "\n",
    "print('x_train length: {} shape: {}'.format(len(x_train), x_train.shape))\n",
    "print('y_train length: {} shape: {}'.format(len(y_train), y_train.shape))\n",
    "print('x_val length: {} shape: {}'.format(len(x_val), x_val.shape))\n",
    "print('y_val length: {} shape: {}'.format(len(y_val), y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "\n",
    "# Compare with simple LSTM Model, according to several sources LSTM is suitable for modeling time series data\n",
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(8, input_shape=x_train.shape[-2:]), \n",
    "        tf.keras.layers.Dense(future_target)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further reading:**\n",
    "    \n",
    "    1. https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\n",
    "    2. https://stackoverflow.com/questions/45943675/meaning-of-validation-steps-in-keras-sequential-fit-generator-parameter-list/45944225\n",
    "    3. https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "datatrain = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "datatrain = datatrain.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "dataval = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "dataval = dataval.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "STEPS_PER_EPOCH = 200 # number of time steps\n",
    "VALIDATION_STEPS = 50\n",
    "\n",
    "# Train the model\n",
    "model_history = model.fit(datatrain, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, validation_data=dataval, \n",
    "                      validation_steps=VALIDATION_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_history(model_history, 'loss', 'val_loss', 'Single Step Model Training and Validation Loss')\n",
    "# plot_train_history(model_history, 'mse', 'val_mse', 'Single Step Model Training and Validation Loss')\n",
    "# plot_train_history(model_history, 'mae', 'val_mae', 'Single Step Model Training and Validation Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
